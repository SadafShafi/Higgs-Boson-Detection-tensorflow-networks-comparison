{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/higgs-boson/training/shard_08.tfrecord\n/kaggle/input/higgs-boson/training/shard_09.tfrecord\n/kaggle/input/higgs-boson/training/shard_12.tfrecord\n/kaggle/input/higgs-boson/training/shard_19.tfrecord\n/kaggle/input/higgs-boson/training/shard_11.tfrecord\n/kaggle/input/higgs-boson/training/shard_04.tfrecord\n/kaggle/input/higgs-boson/training/shard_07.tfrecord\n/kaggle/input/higgs-boson/training/shard_10.tfrecord\n/kaggle/input/higgs-boson/training/shard_14.tfrecord\n/kaggle/input/higgs-boson/training/shard_03.tfrecord\n/kaggle/input/higgs-boson/training/shard_01.tfrecord\n/kaggle/input/higgs-boson/training/shard_00.tfrecord\n/kaggle/input/higgs-boson/training/shard_13.tfrecord\n/kaggle/input/higgs-boson/training/shard_06.tfrecord\n/kaggle/input/higgs-boson/training/shard_20.tfrecord\n/kaggle/input/higgs-boson/training/shard_21.tfrecord\n/kaggle/input/higgs-boson/training/shard_23.tfrecord\n/kaggle/input/higgs-boson/training/shard_05.tfrecord\n/kaggle/input/higgs-boson/training/shard_02.tfrecord\n/kaggle/input/higgs-boson/training/shard_18.tfrecord\n/kaggle/input/higgs-boson/training/shard_16.tfrecord\n/kaggle/input/higgs-boson/training/shard_17.tfrecord\n/kaggle/input/higgs-boson/training/shard_15.tfrecord\n/kaggle/input/higgs-boson/training/shard_22.tfrecord\n/kaggle/input/higgs-boson/validation/shard_04.tfrecord\n/kaggle/input/higgs-boson/validation/shard_07.tfrecord\n/kaggle/input/higgs-boson/validation/shard_03.tfrecord\n/kaggle/input/higgs-boson/validation/shard_01.tfrecord\n/kaggle/input/higgs-boson/validation/shard_00.tfrecord\n/kaggle/input/higgs-boson/validation/shard_06.tfrecord\n/kaggle/input/higgs-boson/validation/shard_05.tfrecord\n/kaggle/input/higgs-boson/validation/shard_02.tfrecord\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# TensorFlow\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"Tensorflow version \" + tf.__version__)\n\n# TF 2.3 version\n# Detect and init the TPU\n# try: # detect TPUs\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n#     strategy = tf.distribute.TPUStrategy(tpu)\n# except ValueError: # detect GPUs\n#     strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n# TF 2.2 version\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","execution_count":3,"outputs":[{"output_type":"stream","text":"Tensorflow version 2.2.0\nNumber of accelerators:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_decoder(feature_description):\n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    return decoder\n\ndef load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    return dataset","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size // batch_size\nvalidation_steps = validation_size // batch_size\n\n# For model.compile\nsteps_per_execution = steps_per_epoch","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\ndecoder = make_decoder(feature_description)\n\ndata_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n\nds_train = load_dataset(train_files, decoder, ordered=False)\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\nepo = 50","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wide and Deep model (by tensorflow team)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make\n\n\n# Wide Network\nwide = keras.experimental.LinearModel()\n\n# Deep Network\ninputs = keras.Input(shape=[28])\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\noutputs = layers.Dense(1)(x)\ndeep = keras.Model(inputs=inputs, outputs=outputs)\n\n# Wide and Deep Network\nwide_and_deep = keras.experimental.WideDeepModel(\n    linear_model=wide,\n    dnn_model=deep,\n    activation='sigmoid',\n)\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n#     experimental_steps_per_execution=steps_per_execution,\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)\n\nhistory = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=epo,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# simple model with AUC instead of accuracy"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = keras.Sequential(\n    [   \n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(150, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(150,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer3\",activation='sigmoid'),\n    ]\n)\n\nfrom keras import losses \nfrom keras import optimizers \nfrom keras import metrics \n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['AUC','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs = epo,steps_per_epoch=steps_per_epoch)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","execution_count":7,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c72a8c0c1bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m model.compile(loss = 'binary_crossentropy',  \n\u001b[1;32m     18\u001b[0m    optimizer = 'adam', metrics = ['AUC','binary_accuracy'])\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mhistory_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"# Simple Model"},{"metadata":{},"cell_type":"markdown","source":"**try simple model with cross entropy and AUC this time**"},{"metadata":{},"cell_type":"markdown","source":"**use your wide and deep model together putting them both in wide_and_deep.fit()"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = keras.Sequential(\n    [   \n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(150, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(150,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer3\",activation='sigmoid'),\n    ]\n)\n\nfrom keras import losses \nfrom keras import optimizers \nfrom keras import metrics \n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs = epo,steps_per_epoch=steps_per_epoch)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Highly Dense, wide and deep Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential(\n    [\n        layers.Dense(1000, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(1000,)),\n        layers.Dense(750, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(750,)),\n        layers.BatchNormalization(),\n        layers.Dense(500, name=\"layer3\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(500,)),\n        layers.BatchNormalization(),\n        layers.Dense(200, activation=\"relu\", name=\"layer4\"),\n        layers.Dropout(0.2, input_shape=(200,)),\n        layers.BatchNormalization(),\n        layers.Dense(100, activation=\"relu\", name=\"layer5\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(50, name=\"layer6\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(50,)),\n        layers.BatchNormalization(),\n        layers.Dense(10, activation=\"relu\", name=\"layer7\"),\n        layers.Dropout(0.2, input_shape=(10,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer8\",activation='sigmoid'),\n    ])\n\nfrom keras import losses \nfrom keras import optimizers \nfrom keras import metrics \n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wide model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential(\n    [\n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(75, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(75,)),\n        layers.BatchNormalization(),\n        layers.Dense(50, name=\"layer3\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(50,)),\n        layers.BatchNormalization(),\n        layers.Dense(20, activation=\"relu\", name=\"layer4\"),\n        layers.Dropout(0.2, input_shape=(20,)),\n        layers.BatchNormalization(),\n        layers.Dense(10, activation=\"relu\", name=\"layer5\"),\n        layers.Dropout(0.2, input_shape=(10,)),\n        layers.BatchNormalization(),\n        layers.Dense(5, name=\"layer6\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(5,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer7\",activation='sigmoid'),\n    ]\n)\nwide=model\n# from keras import losses \n# from keras import optimizers \n# from keras import metrics \n\n# model.compile(loss = 'binary_crossentropy',  \n#    optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\n# history = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)\n\n# history_frame = pd.DataFrame(history.history)\n# history_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\n# history_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential(\n    [\n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer3\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer4\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer5\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer6\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer7\",activation='sigmoid'),\n    ]\n)\ndeep = model\n\n# from keras import losses \n# from keras import optimizers \n# from keras import metrics \n\n# model.compile(loss = 'binary_crossentropy',  \n#    optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\n# history = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)\n\n# history_frame = pd.DataFrame(history.history)\n# history_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\n# history_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# wide and deep model using my two arcitectures"},{"metadata":{"trusted":true},"cell_type":"code","source":"wide_and_deep = keras.experimental.WideDeepModel(\n    linear_model=wide,\n    dnn_model=deep,\n    activation='sigmoid',\n)\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n#     experimental_steps_per_execution=steps_per_execution,\n)\n\nhistory = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n640/640 [==============================] - 247s 386ms/step - loss: 0.6550 - auc: 0.6936 - binary_accuracy: 0.5300 - val_loss: 0.6194 - val_auc: 0.7517 - val_binary_accuracy: 0.5304\nEpoch 2/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.6138 - auc: 0.7594 - binary_accuracy: 0.5300 - val_loss: 0.6017 - val_auc: 0.7760 - val_binary_accuracy: 0.5304\nEpoch 3/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.6046 - auc: 0.7738 - binary_accuracy: 0.5305 - val_loss: 0.5951 - val_auc: 0.7907 - val_binary_accuracy: 0.5304\nEpoch 4/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5997 - auc: 0.7808 - binary_accuracy: 0.5316 - val_loss: 0.5909 - val_auc: 0.7966 - val_binary_accuracy: 0.5305\nEpoch 5/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5968 - auc: 0.7853 - binary_accuracy: 0.5341 - val_loss: 0.5889 - val_auc: 0.7998 - val_binary_accuracy: 0.5313\nEpoch 6/50\n640/640 [==============================] - 169s 263ms/step - loss: 0.5948 - auc: 0.7885 - binary_accuracy: 0.5381 - val_loss: 0.5864 - val_auc: 0.8003 - val_binary_accuracy: 0.5312\nEpoch 7/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5931 - auc: 0.7910 - binary_accuracy: 0.5428 - val_loss: 0.5860 - val_auc: 0.8022 - val_binary_accuracy: 0.5324\nEpoch 8/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.5921 - auc: 0.7924 - binary_accuracy: 0.5474 - val_loss: 0.5859 - val_auc: 0.8060 - val_binary_accuracy: 0.5346\nEpoch 9/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5910 - auc: 0.7941 - binary_accuracy: 0.5514 - val_loss: 0.5845 - val_auc: 0.8058 - val_binary_accuracy: 0.5365\nEpoch 10/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5902 - auc: 0.7954 - binary_accuracy: 0.5554 - val_loss: 0.5838 - val_auc: 0.8070 - val_binary_accuracy: 0.5378\nEpoch 11/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5894 - auc: 0.7965 - binary_accuracy: 0.5581 - val_loss: 0.5828 - val_auc: 0.8047 - val_binary_accuracy: 0.5392\nEpoch 12/50\n640/640 [==============================] - 169s 263ms/step - loss: 0.5888 - auc: 0.7972 - binary_accuracy: 0.5594 - val_loss: 0.5824 - val_auc: 0.8067 - val_binary_accuracy: 0.5378\nEpoch 13/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5882 - auc: 0.7986 - binary_accuracy: 0.5619 - val_loss: 0.5823 - val_auc: 0.8041 - val_binary_accuracy: 0.5414\nEpoch 14/50\n640/640 [==============================] - 167s 261ms/step - loss: 0.5877 - auc: 0.7993 - binary_accuracy: 0.5609 - val_loss: 0.5825 - val_auc: 0.8082 - val_binary_accuracy: 0.5458\nEpoch 15/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5872 - auc: 0.8006 - binary_accuracy: 0.5624 - val_loss: 0.5820 - val_auc: 0.8061 - val_binary_accuracy: 0.5498\nEpoch 16/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5867 - auc: 0.8011 - binary_accuracy: 0.5646 - val_loss: 0.5807 - val_auc: 0.8062 - val_binary_accuracy: 0.5441\nEpoch 17/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.5863 - auc: 0.8017 - binary_accuracy: 0.5645 - val_loss: 0.5810 - val_auc: 0.8096 - val_binary_accuracy: 0.5483\nEpoch 18/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5860 - auc: 0.8023 - binary_accuracy: 0.5665 - val_loss: 0.5806 - val_auc: 0.8058 - val_binary_accuracy: 0.5651\nEpoch 19/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5856 - auc: 0.8028 - binary_accuracy: 0.5669 - val_loss: 0.5812 - val_auc: 0.8084 - val_binary_accuracy: 0.5568\nEpoch 20/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5854 - auc: 0.8035 - binary_accuracy: 0.5695 - val_loss: 0.5800 - val_auc: 0.8071 - val_binary_accuracy: 0.5580\nEpoch 21/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5850 - auc: 0.8039 - binary_accuracy: 0.5695 - val_loss: 0.5802 - val_auc: 0.8064 - val_binary_accuracy: 0.5566\nEpoch 22/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5847 - auc: 0.8044 - binary_accuracy: 0.5712 - val_loss: 0.5791 - val_auc: 0.8141 - val_binary_accuracy: 0.5553\nEpoch 23/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5845 - auc: 0.8048 - binary_accuracy: 0.5726 - val_loss: 0.5810 - val_auc: 0.8073 - val_binary_accuracy: 0.5840\nEpoch 24/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5842 - auc: 0.8051 - binary_accuracy: 0.5752 - val_loss: 0.5788 - val_auc: 0.8116 - val_binary_accuracy: 0.5634\nEpoch 25/50\n640/640 [==============================] - 169s 265ms/step - loss: 0.5840 - auc: 0.8056 - binary_accuracy: 0.5744 - val_loss: 0.5797 - val_auc: 0.8075 - val_binary_accuracy: 0.5640\nEpoch 26/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5838 - auc: 0.8059 - binary_accuracy: 0.5755 - val_loss: 0.5789 - val_auc: 0.8094 - val_binary_accuracy: 0.5748\nEpoch 27/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5836 - auc: 0.8060 - binary_accuracy: 0.5779 - val_loss: 0.5787 - val_auc: 0.8091 - val_binary_accuracy: 0.5816\nEpoch 28/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5834 - auc: 0.8064 - binary_accuracy: 0.5829 - val_loss: 0.5804 - val_auc: 0.8079 - val_binary_accuracy: 0.5795\nEpoch 29/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5832 - auc: 0.8067 - binary_accuracy: 0.5853 - val_loss: 0.5784 - val_auc: 0.8084 - val_binary_accuracy: 0.5957\nEpoch 30/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5831 - auc: 0.8069 - binary_accuracy: 0.5861 - val_loss: 0.5800 - val_auc: 0.8079 - val_binary_accuracy: 0.6069\nEpoch 31/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5828 - auc: 0.8074 - binary_accuracy: 0.5888 - val_loss: 0.5792 - val_auc: 0.8089 - val_binary_accuracy: 0.5919\nEpoch 32/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5826 - auc: 0.8078 - binary_accuracy: 0.5906 - val_loss: 0.5782 - val_auc: 0.8096 - val_binary_accuracy: 0.5970\nEpoch 33/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5825 - auc: 0.8077 - binary_accuracy: 0.5885 - val_loss: 0.5783 - val_auc: 0.8134 - val_binary_accuracy: 0.5936\nEpoch 34/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5823 - auc: 0.8080 - binary_accuracy: 0.5929 - val_loss: 0.5792 - val_auc: 0.8083 - val_binary_accuracy: 0.6218\nEpoch 35/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.5822 - auc: 0.8083 - binary_accuracy: 0.5946 - val_loss: 0.5786 - val_auc: 0.8075 - val_binary_accuracy: 0.6229\nEpoch 36/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.5820 - auc: 0.8084 - binary_accuracy: 0.5951 - val_loss: 0.5791 - val_auc: 0.8127 - val_binary_accuracy: 0.6051\nEpoch 37/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5818 - auc: 0.8089 - binary_accuracy: 0.5977 - val_loss: 0.5772 - val_auc: 0.8130 - val_binary_accuracy: 0.6149\nEpoch 38/50\n640/640 [==============================] - 169s 264ms/step - loss: 0.5818 - auc: 0.8089 - binary_accuracy: 0.5995 - val_loss: 0.5786 - val_auc: 0.8082 - val_binary_accuracy: 0.6157\nEpoch 39/50\n640/640 [==============================] - 168s 262ms/step - loss: 0.5816 - auc: 0.8092 - binary_accuracy: 0.6011 - val_loss: 0.5787 - val_auc: 0.8079 - val_binary_accuracy: 0.6146\nEpoch 40/50\n640/640 [==============================] - 168s 263ms/step - loss: 0.5814 - auc: 0.8091 - binary_accuracy: 0.6013 - val_loss: 0.5777 - val_auc: 0.8136 - val_binary_accuracy: 0.6310\nEpoch 41/50\n633/640 [============================>.] - ETA: 1s - loss: 0.5813 - auc: 0.8096 - binary_accuracy: 0.6056","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers\nnum_classes = 5\n\nmodel = tf.keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/on_device_vision/classifier/landmarks_classifier_oceania_antarctica_V1/1\", output_shape=[2048],\n                   trainable=False),  # Can be True, see below.\n    tf.keras.layers.Dense(1, activation='softmax')\n])\n\n\n# model.build([None, 299, 299, 3])  # Batch input shape.\n\n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow.compat.v2 as tf\n# import tensorflow_hub as hub\n\n# m = hub.KerasLayer('https://tfhub.dev/google/on_device_vision/classifier/landmarks_classifier_oceania_antarctica_V1/1')\n# m = tf.keras.Sequential([\n#     m,\n#     tf.keras.layers.Dense(2, activation='softmax'),\n# ])\n\n\n# m.compile(loss = 'binary_crossentropy',  \n#    optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\n# history = m.fit(ds_train,validation_data=ds_valid, epochs =12 ,steps_per_epoch=13)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}